# CLeVeR: Multi-modal Contrastive Learning for Vulnerability Code Representation
[ACL 2025 Findings, Long Paper]
## Highlight
CLeVeR is a novel approach that leverages contrastive learning to generate precise vulnerability code representations under the supervision of vulnerability descriptions. Through CLIP's distinctive dual-encoder architecture and contrastive loss, CLeVeR establishes fine-grained semantic alignment between code and vulnerability descriptions. Specifically, CLeVeR introduces an Adapter, a Representation Refinement module, and a Description Simulator to mitigate the challenges of semantic misalignment and imbalance between code and descriptions, and input data inconsistency between pre-training and fine-tuning stages, respectively. The vulnerability code representation generated by CLeVeR can be used in various vulnerability-related tasks.

## How to use
### Step 0: Data Collection

### Step 1: Preprocessing
Preprocess the dataset dataset.jsonl and split it into dataset_train.pkl and dataset_test.pkl
